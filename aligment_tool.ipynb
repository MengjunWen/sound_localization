{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from cv2 import aruco\n",
    "\n",
    "with np.load('Obtain_Groundtruth_location\\\\calibration_data.npz') as data:\n",
    "    mtx = data['mtx']\n",
    "    dist = data['dist']\n",
    "def detect_position(image, mtx, dist):\n",
    "    x = 250  # Side length of the square\n",
    "    pts_known = np.array([[-x/2, -x/2], [x/2, -x/2], [x/2, x/2], [-x/2, x/2]], dtype='float32')\n",
    "    frame_width = image.shape[1]\n",
    "    frame_height = image.shape[0]\n",
    "\n",
    "    # Define the ArUco marker dictionary\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n",
    "    parameters = aruco.DetectorParameters()\n",
    "\n",
    "    # Undistort the image using calibration data\n",
    "    # undistorted_frame = cv2.undistort(image, mtx, dist)\n",
    "    frame = image\n",
    "\n",
    "    # Convert to grayscale for marker detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ArUco markers\n",
    "    detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "    corners, ids, _ = detector.detectMarkers(gray)\n",
    "\n",
    "    # if ids is not None:\n",
    "    #     img = cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "    #     for corner in corners:\n",
    "    #         center = np.mean(corner[0], axis=0)\n",
    "    #         cv2.circle(img, tuple(center.astype(int)), 5, (0, 255, 0), -1)  # Draw a circle at the center of the marker\n",
    "\n",
    "    # cv2.imshow('ArUco Markers', img)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    if ids is not None:\n",
    "        \n",
    "        ids = ids.flatten()  # Flatten to a 1D array\n",
    "        if len(ids) < 5:\n",
    "            return None\n",
    "        pts_detected = np.zeros((4, 2), dtype='float32')\n",
    "        for i, marker_id in enumerate([0, 1, 2, 3]):\n",
    "            idx = np.where(ids == marker_id)[0][0]\n",
    "            pts_detected[i] = np.mean(corners[idx][0], axis=0)\n",
    "        \n",
    "        # Compute homography matrix\n",
    "        H, _ = cv2.findHomography(pts_known, pts_detected)\n",
    "\n",
    "        virtual_points = []\n",
    "        # Process each detected marker\n",
    "        for i, marker_id in enumerate(ids):\n",
    "            if marker_id != 4:\n",
    "                continue\n",
    "            c = corners[i][0]\n",
    "            center = np.mean(c, axis=0)\n",
    "            # Transform marker coordinates to virtual plane\n",
    "            pts_img = np.array([[center[0], center[1]]], dtype='float32')\n",
    "            \n",
    "            return cv2.perspectiveTransform(np.array([pts_img]), np.linalg.inv(H))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b115ddfb04345189298116e588a66e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Subfolders:', options=('exp_1', 'exp_10', 'exp_11', 'exp_12', 'exp_13', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera parameters not found. Using default values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 219.\n",
      "No ArUco marker detected in frame 220.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 222.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 294.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 753.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 1142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 1216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 1314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 1374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ArUco marker detected in frame 1379.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location data saved to location_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Video, HTML\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# --- Global Variables ---\n",
    "current_frame = 0\n",
    "cap = None\n",
    "video_path = None\n",
    "total_frames = 0\n",
    "fps = 30  # Default FPS\n",
    "start_frame = 50\n",
    "stop_frame = 100\n",
    "mtx = None # Placeholder for camera matrix\n",
    "dist = None # Placeholder for distortion coefficients\n",
    "\n",
    "experiments_folder = 'experiments/experiments'\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_frame(frame_num, cap):\n",
    "    \"\"\"Reads a specific frame from the video.\"\"\"\n",
    "    global total_frames\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading frame or end of video.\")\n",
    "        return None\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    _, encoded_frame = cv2.imencode('.jpg', frame)\n",
    "    return encoded_frame.tobytes()\n",
    "\n",
    "def update_video_display(frame_data, video_widget):\n",
    "    \"\"\"Updates the displayed video frame.\"\"\"\n",
    "    video_widget.value = frame_data\n",
    "    with current_frame_output:\n",
    "        current_frame_output.clear_output(wait=True)\n",
    "        display(format_frame_info(current_frame, fps))\n",
    "\n",
    "def format_frame_info(frame_num, fps):\n",
    "    \"\"\"Formats frame number and time information.\"\"\"\n",
    "    if fps > 0:  # Avoid division by zero\n",
    "        seconds = frame_num / fps\n",
    "        return f\"Frame: {frame_num} ({seconds:.2f}s)\"\n",
    "    else:\n",
    "        return f\"Frame: {frame_num} (FPS not available)\"\n",
    "\n",
    "# --- UI Elements ---\n",
    "\n",
    "# Get all subfolders\n",
    "subfolders = [f.name for f in os.scandir(experiments_folder) if f.is_dir()]\n",
    "\n",
    "# Dropdown\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=subfolders,\n",
    "    description='Subfolders:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Video Widget\n",
    "video_widget = widgets.Image(format='jpeg', width=640, height=480)\n",
    "\n",
    "# Current Frame Display\n",
    "current_frame_output = widgets.Output()\n",
    "\n",
    "# Buttons\n",
    "forward_button = widgets.Button(description=\">>\")\n",
    "backward_button = widgets.Button(description=\"<<\")\n",
    "forward_1s_button = widgets.Button(description=\"> 1s\")\n",
    "backward_1s_button = widgets.Button(description=\"< 1s\")\n",
    "forward_60s_button = widgets.Button(description=\"> 60s\")\n",
    "backward_60s_button = widgets.Button(description=\"< 60s\")\n",
    "load_button = widgets.Button(description=\"Load Video\")\n",
    "start_frame_button = widgets.Button(description=\"Set Start\")\n",
    "stop_frame_button = widgets.Button(description=\"Set Stop\")\n",
    "transform_frame_button = widgets.Button(description=\"Transform Image\")\n",
    "extract_location_button = widgets.Button(description=\"Extract Location\")\n",
    "# Output for Start/Stop Frame Information\n",
    "start_frame_output = widgets.Output()\n",
    "stop_frame_output = widgets.Output()\n",
    "\n",
    "# --- Event Handlers ---\n",
    "\n",
    "def on_load_clicked(b):\n",
    "    \"\"\"Loads the video.\"\"\"\n",
    "    global cap, video_path, current_frame, fps, total_frames, start_frame, stop_frame, mtx, dist\n",
    "    current_frame = 0\n",
    "    start_frame = 50\n",
    "    stop_frame = 1500\n",
    "    \n",
    "\n",
    "    files = [f.name for f in os.scandir(os.path.join(experiments_folder, dropdown.value)) if f.is_file()]\n",
    "    video_file = next((f for f in files if f.endswith('.mp4')), None)\n",
    "\n",
    "    if video_file:\n",
    "        video_path = os.path.join(experiments_folder, dropdown.value, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video.\")\n",
    "            return\n",
    "        \n",
    "        # Load camera parameters if available\n",
    "        param_file = os.path.join(experiments_folder, dropdown.value, 'c922_params.npz')\n",
    "        if os.path.exists(param_file):\n",
    "            data = np.load(param_file)\n",
    "            mtx, dist = data['mtx'], data['dist']\n",
    "        else:\n",
    "            print(\"Camera parameters not found. Using default values.\")\n",
    "\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        initial_frame = get_frame(current_frame, cap)\n",
    "        if initial_frame:\n",
    "            update_video_display(initial_frame, video_widget)\n",
    "\n",
    "        # Reset start/stop frame info\n",
    "        with start_frame_output:\n",
    "            start_frame_output.clear_output()\n",
    "            print(format_frame_info(start_frame, fps))\n",
    "        with stop_frame_output:\n",
    "            stop_frame_output.clear_output()\n",
    "            print(format_frame_info(stop_frame, fps))\n",
    "\n",
    "    else:\n",
    "        video_path = None\n",
    "        print('No video file found.')\n",
    "\n",
    "def on_forward_clicked(b):\n",
    "    \"\"\"Moves forward by one frame.\"\"\"\n",
    "    global current_frame, total_frames\n",
    "    if cap and current_frame < total_frames - 1:\n",
    "        current_frame += 1\n",
    "        frame_data = get_frame(current_frame, cap)\n",
    "        if frame_data:\n",
    "            update_video_display(frame_data, video_widget)\n",
    "\n",
    "def on_backward_clicked(b):\n",
    "    \"\"\"Moves backward by one frame.\"\"\"\n",
    "    global current_frame\n",
    "    if cap and current_frame > 0:\n",
    "        current_frame -= 1\n",
    "        frame_data = get_frame(current_frame, cap)\n",
    "        if frame_data:\n",
    "            update_video_display(frame_data, video_widget)\n",
    "\n",
    "def on_forward_1s_clicked(b):\n",
    "    \"\"\"Moves forward by 1 second.\"\"\"\n",
    "    global current_frame, fps, total_frames\n",
    "    if cap:\n",
    "        current_frame += int(fps)\n",
    "        current_frame = min(current_frame, total_frames - 1)\n",
    "        frame_data = get_frame(current_frame, cap)\n",
    "        if frame_data:\n",
    "            update_video_display(frame_data, video_widget)\n",
    "\n",
    "def on_backward_1s_clicked(b):\n",
    "    \"\"\"Moves backward by 1 second.\"\"\"\n",
    "    global current_frame, fps\n",
    "    if cap:\n",
    "        current_frame -= int(fps)\n",
    "        current_frame = max(current_frame, 0)\n",
    "        frame_data = get_frame(current_frame, cap)\n",
    "        if frame_data:\n",
    "            update_video_display(frame_data, video_widget)\n",
    "\n",
    "def on_forward_60s_clicked(b):\n",
    "    \"\"\"Moves forward by 60 seconds.\"\"\"\n",
    "    global current_frame, fps, total_frames\n",
    "    if cap:\n",
    "        current_frame += int(fps * 60)\n",
    "        current_frame = min(current_frame, total_frames - 1)\n",
    "        frame_data = get_frame(current_frame, cap)\n",
    "        if frame_data:\n",
    "            update_video_display(frame_data, video_widget)\n",
    "\n",
    "def on_backward_60s_clicked(b):\n",
    "    \"\"\"Moves backward by 60 seconds.\"\"\"\n",
    "    global current_frame, fps\n",
    "    if cap:\n",
    "        current_frame -= int(fps * 60)\n",
    "        current_frame = max(current_frame, 0)\n",
    "        frame_data = get_frame(current_frame, cap)\n",
    "        if frame_data:\n",
    "            update_video_display(frame_data, video_widget)\n",
    "\n",
    "def on_start_frame_clicked(b):\n",
    "    \"\"\"Sets the start frame.\"\"\"\n",
    "    global start_frame\n",
    "    start_frame = current_frame\n",
    "    with start_frame_output:\n",
    "        start_frame_output.clear_output()\n",
    "        print(format_frame_info(start_frame, fps))\n",
    "\n",
    "def on_stop_frame_clicked(b):\n",
    "    \"\"\"Sets the stop frame.\"\"\"\n",
    "    global stop_frame\n",
    "    stop_frame = current_frame\n",
    "    with stop_frame_output:\n",
    "        stop_frame_output.clear_output()\n",
    "        print(format_frame_info(stop_frame, fps))\n",
    "\n",
    "def on_transform_image_clicked(b):\n",
    "    \"\"\"Transforms the image.\"\"\"\n",
    "    global current_frame\n",
    "    if cap:\n",
    "        frame_data = get_frame(current_frame, cap)\n",
    "        if frame_data:\n",
    "            frame = cv2.imdecode(np.frombuffer(frame_data, np.uint8), cv2.IMREAD_COLOR)\n",
    "            center_location = detect_position(frame, mtx, dist)\n",
    "            if center_location is not None:\n",
    "                _, encoded_frame = cv2.imencode('.jpg', frame)\n",
    "                update_video_display(encoded_frame.tobytes(), video_widget)\n",
    "\n",
    "def on_extract_location_clicked(b):\n",
    "    \"\"\"Extracts the location of the sound source.\"\"\"\n",
    "    global current_frame, start_frame, stop_frame\n",
    "    location_data = []\n",
    "    if cap:\n",
    "        for frame_num in range(start_frame, stop_frame + 1):\n",
    "            current_frame = frame_num\n",
    "            frame_data = get_frame(current_frame, cap)\n",
    "            if frame_data:\n",
    "                frame = cv2.imdecode(np.frombuffer(frame_data, np.uint8), cv2.IMREAD_COLOR)\n",
    "                center_location = detect_position(frame, mtx, dist)\n",
    "                if center_location is not None:\n",
    "                    _, encoded_frame = cv2.imencode('.jpg', frame)\n",
    "                    update_video_display(encoded_frame.tobytes(), video_widget)\n",
    "                    location_data.append((frame_num, center_location))\n",
    "                else:\n",
    "                    print(f\"No ArUco marker detected in frame {frame_num}.\")\n",
    "\n",
    "    if len(location_data) > 0:\n",
    "        with open('location_data.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Frame Number', 'X', 'Y'])\n",
    "            for frame_num, loc in location_data:\n",
    "                x = loc[0][0][0]  # Access the x coordinate\n",
    "                y = loc[0][0][1]  # Access the y coordinate\n",
    "                writer.writerow([frame_num, x, y])\n",
    "        print(\"Location data saved to location_data.csv.\")\n",
    "        \n",
    "# --- Attach Event Handlers ---\n",
    "\n",
    "load_button.on_click(on_load_clicked)\n",
    "forward_button.on_click(on_forward_clicked)\n",
    "backward_button.on_click(on_backward_clicked)\n",
    "forward_1s_button.on_click(on_forward_1s_clicked)\n",
    "backward_1s_button.on_click(on_backward_1s_clicked)\n",
    "forward_60s_button.on_click(on_forward_60s_clicked)\n",
    "backward_60s_button.on_click(on_backward_60s_clicked)\n",
    "start_frame_button.on_click(on_start_frame_clicked)\n",
    "stop_frame_button.on_click(on_stop_frame_clicked)\n",
    "transform_frame_button.on_click(on_transform_image_clicked)\n",
    "extract_location_button.on_click(on_extract_location_clicked)\n",
    "# --- Layout ---\n",
    "\n",
    "buttons_60s = widgets.HBox([backward_60s_button, forward_60s_button])\n",
    "buttons_1s = widgets.HBox([backward_1s_button, forward_1s_button])\n",
    "buttons_frames = widgets.HBox([backward_button, forward_button])\n",
    "buttons_start_stop = widgets.HBox([start_frame_button, stop_frame_button])\n",
    "info_layout = widgets.HBox([\n",
    "    widgets.VBox([widgets.Label(\"Start Frame:\"), start_frame_output]),\n",
    "    widgets.VBox([widgets.Label(\"Stop Frame:\"), stop_frame_output]),\n",
    "])\n",
    "\n",
    "ui_layout = widgets.VBox([\n",
    "    dropdown,\n",
    "    load_button,\n",
    "    video_widget,\n",
    "    current_frame_output,\n",
    "    buttons_60s,\n",
    "    buttons_1s,\n",
    "    buttons_frames,\n",
    "    buttons_start_stop,\n",
    "    info_layout,\n",
    "    transform_frame_button,\n",
    "    extract_location_button,\n",
    "])\n",
    "\n",
    "display(ui_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40f03b693b44e18aca5a155e6aea7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Subfolders:', options=('exp_1', 'exp_10', 'exp_11', 'exp_12', 'exp_13', 'exp_14', 'exp_1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c40cf44485b477fb60202dfbb22edec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Distance microphone 1 (cm):'), FloatText(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e280f366e14b2a9fafdaadc9b6cc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Distance microphone 2 (cm):'), FloatText(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457ef1b6840d478d9019557a810405ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Distance microphone 3 (cm):'), FloatText(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2458f9abe043699074848474255800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Distance microphone 4 (cm):'), FloatText(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb9ef3dcfee4b1c8457e079877a9212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "experiments_folder = 'experiments/experiments'\n",
    "subfolders = [f.name for f in os.scandir(experiments_folder) if f.is_dir()]\n",
    "\n",
    "# Dropdown\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=subfolders,\n",
    "    description='Subfolders:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Output widget for displaying waveforms and audio players\n",
    "out = widgets.Output()\n",
    "\n",
    "def on_dropdown_change(change):\n",
    "    \"\"\"Handles the dropdown value change.\"\"\"\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        audio_files = [f.name for f in os.scandir(os.path.join(experiments_folder, dropdown.value, 'audio_wav_files')) if f.is_file() and f.name.endswith('.wav')]\n",
    "\n",
    "        if len(audio_files) < 4:\n",
    "            print('Not enough audio files found.')\n",
    "            return\n",
    "\n",
    "        for i, audio_file in enumerate(audio_files[:4]):\n",
    "            audio_path = os.path.join(experiments_folder, dropdown.value, 'audio_wav_files', audio_file)\n",
    "            try:\n",
    "                with wave.open(audio_path, 'r') as wf:\n",
    "                    num_channels = wf.getnchannels()\n",
    "                    framerate = wf.getframerate()\n",
    "                    num_frames = wf.getnframes()\n",
    "\n",
    "                    frames = wf.readframes(num_frames)\n",
    "\n",
    "                    if wf.getsampwidth() == 2:\n",
    "                        audio_data = np.frombuffer(frames, dtype=np.int16)\n",
    "                    elif wf.getsampwidth() == 1:\n",
    "                        audio_data = np.frombuffer(frames, dtype=np.int8)\n",
    "                    else:\n",
    "                        raise ValueError(\"Unsupported sample width. Only 8-bit and 16-bit are currently supported.\")\n",
    "\n",
    "                    audio_data = audio_data / np.max(np.abs(audio_data))\n",
    "                    time = np.linspace(0, num_frames / framerate, num=num_frames)\n",
    "\n",
    "                    # Handle stereo audio: plot left and right channels separately\n",
    "                    if num_channels == 2:\n",
    "                        fig, axs = plt.subplots(2, 2, figsize=(10, 4))\n",
    "\n",
    "                        # Left channel\n",
    "                        left_channel_data = audio_data[::2]\n",
    "                        axs[0, 0].plot(time, left_channel_data)\n",
    "                        axs[0, 0].set(title=f'Waveform of {audio_file} (Left Channel)')\n",
    "                        axs[0, 0].set(xlabel='Time (s)', ylabel='Amplitude')\n",
    "                        axs[0, 0].label_outer()\n",
    "                        \n",
    "                        # Spectrogram of the left channel\n",
    "                        axs[1, 0].specgram(left_channel_data, Fs=framerate, NFFT=1024, cmap=plt.cm.get_cmap('magma'))\n",
    "                        axs[1, 0].set(title=f'Spectrogram (Left Channel)')\n",
    "                        axs[1, 0].set(xlabel='Time (s)', ylabel='Frequency (Hz)')\n",
    "                        axs[1, 0].label_outer()\n",
    "\n",
    "                        # Display audio player for the left channel\n",
    "                        audio_left_label = widgets.Label(value=\"Audio left:\")\n",
    "                        display(audio_left_label)\n",
    "                        display(Audio(data=left_channel_data, rate=framerate))\n",
    "\n",
    "                        # Right channel\n",
    "                        right_channel_data = audio_data[1::2]\n",
    "                        axs[0, 1].plot(time, right_channel_data)\n",
    "                        axs[0, 1].set(title=f'Waveform of {audio_file} (Right Channel)')\n",
    "                        axs[0, 1].set(xlabel='Time (s)', ylabel='Amplitude')\n",
    "                        axs[0, 1].label_outer()\n",
    "\n",
    "                        # Spectrogram of the right channel\n",
    "                        axs[1, 1].specgram(right_channel_data, Fs=framerate, NFFT=1024, cmap=plt.cm.get_cmap('magma'))\n",
    "                        axs[1, 1].set(title=f'Spectrogram (Right Channel)')\n",
    "                        axs[1, 1].set(xlabel='Time (s)', ylabel='Frequency (Hz)')\n",
    "                        axs[1, 1].label_outer()\n",
    "\n",
    "                        # Display audio player for the right channel\n",
    "                        audio_right_label = widgets.Label(value=\"Audio right:\")\n",
    "                        display(audio_right_label)\n",
    "                        display(Audio(data=right_channel_data, rate=framerate))\n",
    "\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        # For mono, plot normally\n",
    "                        fig, axs = plt.subplots(1, 1, figsize=(10, 2))\n",
    "                        axs.plot(time, audio_data)\n",
    "                        axs.set(title=f'Waveform of {audio_file}')\n",
    "                        axs.set(xlabel='Time (s)', ylabel='Amplitude')\n",
    "                        axs.label_outer()\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "\n",
    "                        # Display audio player for mono\n",
    "                        display(Audio(data=audio_data, rate=framerate))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading or displaying {audio_file}: {e}\")\n",
    "\n",
    "# Distance Input\n",
    "distance_label_1 = widgets.Label(value=\"Distance microphone 1 (cm):\")\n",
    "distance_input_1 = widgets.FloatText(\n",
    "    value=0.0,\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "distance_label_2 = widgets.Label(value=\"Distance microphone 2 (cm):\")\n",
    "distance_input_2 = widgets.FloatText(\n",
    "    value=0.0,\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "distance_label_3 = widgets.Label(value=\"Distance microphone 3 (cm):\")\n",
    "distance_input_3 = widgets.FloatText(\n",
    "    value=0.0,\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "distance_label_4 = widgets.Label(value=\"Distance microphone 4 (cm):\")\n",
    "distance_input_4 = widgets.FloatText(\n",
    "    value=0.0,\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "distance_input_1 = widgets.HBox([distance_label_1, distance_input_1])\n",
    "distance_input_2 = widgets.HBox([distance_label_2, distance_input_2])\n",
    "distance_input_3 = widgets.HBox([distance_label_3, distance_input_3])\n",
    "distance_input_4 = widgets.HBox([distance_label_4, distance_input_4])\n",
    "\n",
    "dropdown.observe(on_dropdown_change, names='value')\n",
    "\n",
    "display(dropdown, distance_input_1, distance_input_2, distance_input_3, distance_input_4, out)\n",
    "\n",
    "on_dropdown_change({'new': dropdown.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted experiments\\experiments\\exp_1\\recorded_20241219_185715_177641.avi to experiments\\experiments\\exp_1\\recorded_20241219_185715_177641.mp4\n",
      "{'exp_1': 'experiments\\\\experiments\\\\exp_1\\\\recorded_20241219_185715_177641.avi'}\n",
      "{'exp_1': 'experiments\\\\experiments\\\\exp_1\\\\recorded_20241219_185715_177641.mp4'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Get all subfolders in the \"experiments\" folder\n",
    "experiments_folder = 'experiments\\\\experiments'  # Correct the path if needed\n",
    "subfolders = [f.name for f in os.scandir(experiments_folder) if f.is_dir()]\n",
    "\n",
    "# Check each subfolder for a file ending with \".avi\"\n",
    "avi_files = {}\n",
    "mp4_files = {}\n",
    "for i, subfolder in enumerate(subfolders):\n",
    "    if i > 0:\n",
    "        break\n",
    "    files = [f.name for f in os.scandir(os.path.join(experiments_folder, subfolder)) if f.is_file()]\n",
    "    avi_file = next((f for f in files if f.endswith('.avi')), None)\n",
    "    if avi_file:\n",
    "        avi_file_path = os.path.join(experiments_folder, subfolder, avi_file)\n",
    "        output_file_path = os.path.splitext(avi_file_path)[0] + '.mp4'\n",
    "\n",
    "        # Use subprocess to run ffmpeg command\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                'C:\\\\Users\\\\calvi\\\\Downloads\\\\ffmpeg-7.1-essentials_build\\\\ffmpeg-7.1-essentials_build\\\\bin\\\\ffmpeg.exe',\n",
    "                '-i', avi_file_path,\n",
    "                '-ac', '2',\n",
    "                '-b:v', '2000k',\n",
    "                '-c:a', 'aac',\n",
    "                '-c:v', 'libx264',\n",
    "                '-b:a', '160k',\n",
    "                '-vprofile', 'high',\n",
    "                '-bf', '0',\n",
    "                '-strict', 'experimental',\n",
    "                '-f', 'mp4',\n",
    "                output_file_path\n",
    "            ], check=True)  # check=True raises an error if ffmpeg fails\n",
    "            avi_files[subfolder] = avi_file_path\n",
    "            mp4_files[subfolder] = output_file_path\n",
    "            print(f\"Successfully converted {avi_file_path} to {output_file_path}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error converting {avi_file_path}: {e}\")\n",
    "\n",
    "print(avi_files)\n",
    "print(mp4_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
